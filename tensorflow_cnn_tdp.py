# -*- coding: utf-8 -*-
"""TensorFlow-CNN-TDP.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1uOyOQMAevz0lVHKSxaMpf4_Wgw1O4TJP

# Importing libraries
"""

import tensorflow as tf
from tensorflow import keras
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import Dropout
from keras import regularizers

import os
import tempfile

import matplotlib as mpl
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import seaborn as sns

import sklearn
from sklearn import preprocessing
from sklearn.metrics import confusion_matrix
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

"""##Importing and showing the deployed dataset"""

df = pd.read_csv("https://raw.githubusercontent.com/Rodrigo190119/TDP-Surveillance_Prediction_Model/main/Dataset.csv")
df.head()

"""##Preparing the Dataset

In this section, we must choose the columns that will be used to train the predictive model and get rid of the ones that are not. In this case, we will use the columns of information collected manually such as "Person_type", "Armed" and "Actions"
"""

cols_to_drop = ['Colour', 'Height', 'Age', 'Gender', 'Head', 'Torso', 'Legs', 'Feet', 'Accessories', 'Blood', 'Banners', 'Type', 'x_min', 'y_min', 'x_max', 'y_max']
df = df.drop(cols_to_drop, axis = 1)
df.head()

"""Next, data used in the predictive model needs to be numerical. We can see that the columns "Person_type" and "Armed" contain classified values. """

df.info()

"""To adapt the data to the model we need, each column value is added as new columns and binary values are assigned to them. """

Person_TypeColumnDummy = pd.get_dummies(df['Person_type'])
Armed_TypeColumnDummy = pd.get_dummies(df['Armed'])
df = pd.concat((df, Person_TypeColumnDummy, Armed_TypeColumnDummy), axis=1)
df.head()

"""Then, original columns are removed."""

df = df.drop(['Person_type', 'Armed'], axis=1)
df.head()

"""The training exercise consists of predicting if a person is considered as delinquent analizing their actions and if they possess a weapon. To quickly assign the columns to be used for training, the columns referring to the type of person should be placed at the end. """

c1 = df.pop('Gun')
c2 = df.pop('Knife')
c3 = df.pop('No')
c4 = df.pop('Other')
df.insert(1, 'Gun', c1)
df.insert(1, 'Knife', c2)
df.insert(1, 'No', c3)
df.insert(1, 'Other', c4)
df.head()

"""We confirm that the data in the dataset has purely numeric values."""

df.info()

"""To analize the dataset this is converted into an array"""

dataset = df.values

dataset

"""Here we define the metrics of the coordinate axes. Notice that the all rows in the dataset are asigned to "X", but we only extract the first 5 columns since those are the columns that are being analized in the future. Then, the column "Delinquent" of the excersise is asigned to the Y axis."""

X = dataset[: ,0:5]
Y = dataset[: ,6]

""" We scale the data using packages from scikit-learn. The min_max_scaler function scales the dataset so the input features are between 0 and 1."""

min_max_scaler = preprocessing.MinMaxScaler()
X_scale = min_max_scaler.fit_transform(X)

X_scale

"""We split the dataset into a test set, validation set and training set. The size of the validation and test will be 30% of the dataset. Then, we split again the validation set and test set so they have the same size.

Now, we have six variables that will be used in the dataset:


*   X_train (70% of dataset, 5 input features)
*   X_val (15% of dataset, 5 input features)
*   X_test (15% of dataset, 5 input features)
*   Y_train (70% of dataset, 1 label features)
*   Y_val (15% of dataset, 1 label features)
*   Y_test (15% of dataset, 1 label features)
"""

X_train, X_val_test, Y_train, Y_val_test = train_test_split(X_scale, Y, test_size=0.3)

X_val, X_test, Y_val, Y_test = train_test_split(X_val_test, Y_val_test, test_size=0.5)

"""This shows the shapes of the arrays (i.e. their dimensions)"""

print(X_train.shape, X_val.shape, X_test.shape, Y_train.shape, Y_val.shape, Y_test.shape)

"""The training set has 1127 data points, the validation set has 241 data points and the test set has 242 data points. This is because the amount of data in the dataset is odd. The X variables have 5 input features, while the Y variables has one feature (the one to predict).

##Building and Training the Neural Network

###CNN

####Setting up the Architecture
"""

model = Sequential([ #the model will be stored in the variable and it will be described sequentially
    Dense(32, activation='relu', input_shape=(5,)), #first dense layer with 32 neurons, RelU activation and 5 inputs
    Dense(32, activation='relu'), #second dense layer with 32 neurons and RelU activation
    Dense(1, activation='sigmoid'), #third dense layer with 1 neuron and sigmoid activation 
])

"""####Choosing algorithm, loss function and metrics

We call the function "model.compile" to configure the model.
"""

model.compile(optimizer='sgd', #using Stochastic Gradient Descent algorithm
              loss='binary_crossentropy', #loss function for outputs (takes values 1 or 0)
              metrics=['accuracy']) #tracks the accuracy

"""The "fit" function sets the parameters to the data. We are using the X_train and Y_train data. The size of the batch is 32 and how long we want to train setting the value of "epochs". The validation data is specified so the model will tell us how we are doing on the validation at each point. This function returns a history wich we are saving as the variable "h"."""

h = model.fit(X_train, Y_train,
          batch_size=32, epochs=100,  
          validation_data=(X_val, Y_val))

"""Visualizing Loss."""

plt.plot(h.history['loss'])
plt.plot(h.history['val_loss'])
plt.title('Model loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Train', 'Val'], loc='upper right')
plt.show()

"""Visualizing Accuracy."""

plt.plot(h.history['accuracy'])
plt.plot(h.history['val_accuracy'])
plt.title('Model accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train', 'Val'], loc='lower right')
plt.show()

"""Evaluating test."""

model.evaluate(X_test, Y_test)[1] #index 1 returns accuracy, index 0 returns loss

"""The accuracy of predicting a delinquent is 88.84%. We can also try different techniques to improve the accuracy using the same variables and input values.

###CNN with Over-fitting and Regularization
"""

model2 = Sequential([
    Dense(1000, activation='relu', input_shape=(5,)),
    Dense(1000, activation='relu'),
    Dense(1000, activation='relu'),
    Dense(1000, activation='relu'),
    Dense(1, activation='sigmoid'),
])

model2.compile(optimizer='adam',
              loss='binary_crossentropy',
              metrics=['accuracy'])
              
h2 = model2.fit(X_train, Y_train,
          batch_size=32, epochs=100,
          validation_data=(X_val, Y_val))

plt.plot(h2.history['accuracy'])
plt.plot(h2.history['val_accuracy'])
plt.title('Model accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train', 'Val'], loc='lower right')
plt.show()

model2.evaluate(X_test, Y_test)[1]

"""###CNN  with Ridge Regression Regularization"""

model3 = Sequential([
    Dense(1000, activation='relu', kernel_regularizer=regularizers.l2(0.01), input_shape=(5,)),
    Dropout(0.3),
    Dense(1000, activation='relu', kernel_regularizer=regularizers.l2(0.01)),
    Dropout(0.3),
    Dense(1000, activation='relu', kernel_regularizer=regularizers.l2(0.01)),
    Dropout(0.3),
    Dense(1000, activation='relu', kernel_regularizer=regularizers.l2(0.01)),
    Dropout(0.3),
    Dense(1, activation='sigmoid', kernel_regularizer=regularizers.l2(0.01)),
])

model3.compile(optimizer='adam',
              loss='binary_crossentropy',
              metrics=['accuracy'])
              
h3 = model3.fit(X_train, Y_train,
          batch_size=32, epochs=100,
          validation_data=(X_val, Y_val))

plt.plot(h3.history['accuracy'])
plt.plot(h3.history['val_accuracy'])
plt.title('Model accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train', 'Val'], loc='lower right')
plt.show()

model3.evaluate(X_test, Y_test)[1]

"""#Conclusions

The accuracy of the 3 different techniques are the following:

*   88.84% for the CNN model
*   89.67% for the CNN with Over-fitting and Regularization model
*   88.84% for the CNN with Ridge Regression Regularization model

Notice that the most accurate model was the Over-Fitting and Regularization model but with a slightly increase percentage. Altough the first and third model obtained the same accuracy percentage, the main difference between them is that the technique used in the third model prevents over-fitting issues.

#Summary

In this research, we coded a prediction model following these steps:

*   We prepared and processed the data
*   Built and Trained a Neural Network
*   Implemented functions to Visualize Loss and Accuracy
*   Added two regularization techniques to the Neural Network
*   Compared the accuracy of the models
"""